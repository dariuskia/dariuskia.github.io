---
title: Manufacturing Dissent
# excerpt: How 
publishDate: 2025-10-30
updatedDate: 2026-02-17  # Optional - only include if you've updated the post
draft: false
isFeatured: false  # Set to true to feature this post on the homepage
tags:
  - social-media
---

The post-industrial, pre-social media age was characterized by gatekeepers—media moguls, mega-corporations, and lurking politicians who held the keys to power. This structure enabled few but massive points of leverage that those powerful individuals could wield, molding the public consciousness into a form friendly to their interests. The advent of social media rattled this structure; platforms like Facebook, Twitter (now X), and the like offered a "marketplace of ideas" in the truest sense—the keys to power were accessible to anyone with a laptop and internet connection. Fast forward a few decades, and rather than widespread social and political coordination, we live in a period of unprecedented [political polarization](https://www.pewresearch.org/short-reads/2022/03/10/the-polarization-in-todays-congress-has-roots-that-go-back-decades/) and [wealth inequality](https://fred.stlouisfed.org/series/WFRBST01134). Social media was effective in inverting these structures of information: in lieu of top-down control, every fringe, grassroots ideology now has its own platform and subreddit. However, the democratization of ideas has come at a cost. Those mechanisms that encouraged self-expression and disrupted hegemony have also fragmented consensus and impaired collective action, enabling the same powerful actors of the past to preserve their grip over the status quo via disorientation and disinformation.

Prior to the internet and social media, ideas concerning culture and politics flowed top-down. Chomsky describes this mechanism at length in Manufacturing Consent. Since the 19th century, as the media industry has become more cost-prohibitive, the involved players have reduced in size and concentrated in power, with a few dozen companies accounting for over half of all published media. This concentration coupled with media's dependence on advertising revenue has enabled corporate power to both directly influence content and whip dissenting publishers into shape, via "flak". These tactics are not exclusive to corporations and advertisers: in the early 2000s, eager to stoke a war with Iraq, government officials cherry-picked intelligence documents, dropped them into the hands of the mass media, and then publicly espoused those same documents, a process called "stovepiping". While dissenting opinions, like anti-war protests, remained, these narratives injected by the elite were largely adopted by the public.

Gaining steam and mass adoption in the early 2000s, social media was poised to be a counterforce to manufactured consent. Social media introduced algorithmic curation: rather than a centralized publisher deciding what content to disseminate, algorithms engineered to individuals' tastes supplanted the role of distribution. Independent influencers and corporations alike are at the mercy of the algorithm: on Facebook, the New York Times' coverage of foreign aid has just as much virality potential as someone's cute cat video. Optimists saw this development as inherently anti-hegemonic. In [A Declaration of the Independence of Cyberspace](https://www.eff.org/cyberspace-independence), John Perry Barlow addresses the ruling elite:
> Your increasingly obsolete information industries would perpetuate themselves by proposing laws, in America and elsewhere, that claim to own speech itself throughout the world. These laws would declare ideas to be another industrial product, no more noble than pig iron. In our world, whatever the human mind may create can be reproduced and distributed infinitely at no cost. The global conveyance of thought no longer requires your factories to accomplish.

Indeed, Barlow's sentiment wasn't entirely chest-puffing. [Breuer et al.](https://web.stanford.edu/class/comm1a/readings/breuer-tunisia.pdf) describe the circuitous way in which the Tunisian protests of 2010-2011  against the then-President, Ben Ali, bypassed government censorship and spread through social media networks, eventually resulting in Ali's deposition.
> Around the globe, these activists now joined efforts to screen Facebook for protest related posts, translating the material, and structuring it into a coherent, chronological narrative . . . Networks like Global Voices and Nawaat started to run special online features covering the protests and spread the word through their own social media channels on Facebook, Twitter, and YouTube. Once the information had been made available in a publishable form, international broadcasters were able to pick it up and re-import it into the country, thus 'leapfrogging' the blackout imposed by Tunisian state-media gatekeepers.

Beyond informing the public, those same social media channels, documenting the large-scale demonstrations, "helped many Tunisians to overcome the barrier of fear that had so far prevented them from taking offline action." The events in Tunisia inspired social media activists in [Egypt](https://boingboing.net/2011/02/02/egypt-the-viral-vlog.html), who staged their own successful uprising against President Hosni Mubarak, and engendered widespread unrest throughout the Middle East and North Africa, known as Arab Spring — ["the revolution will be twittered"](https://www.theatlantic.com/daily-dish/archive/2009/06/the-revolution-will-be-twittered/200478/).

Social media algorithms themselves are incredibly valuable technological feats, worth trillions of dollars in value; however, at a high level, they are not too difficult to understand. [Naranayan](https://knightcolumbia.org/content/understanding-social-media-recommendation-algorithms) describes the express purpose of social media being to maximize engagement. Engagement, depending on the platform, is some weighted sum of the interactions a user has against content they consume. For example, liking a tweet on X is one form of engagement, as is watching a video until completion on YouTube. Social media platforms that run advertisements seek out sustained engagement from users to generate more revenue. Ensembles of statistical and machine learning algorithms process massive amounts of data of user demographics and preferences to recommend engaging content. Naranayan points out how these algorithms are able to infer details like race and gender from its viewers to recommend targeted content, despite never asking for such information.

Social media creates a positive feedback loop between engagement, belief entrenchment, and misinformation. Recently, much criticism has been levied against social media companies for increasing [polarization](https://www.theatlantic.com/ideas/archive/2022/07/social-media-harm-facebook-meta-response/670975/) and [over-representing extremist views](https://www.ft.com/content/9251504e-c60e-4142-b1fb-c86b96275814). There's an intuition behind this: enraging or extremist content is more likely to generate engagement, and therefore is promoted by the algorithm. [Ecker et al.](https://www.nature.com/articles/s44159-021-00006-y) catalogue the drivers of misinformation and cite appeals to emotion as a crucial player, as it "distracts readers from potentially more
diagnostic cues, such as source credibility". In one study, "social judgements about the people featured in headlines were strongly determined by emotional valence of the headline but unaffected by trustworthiness of the news source." 

However, polarizing content and appeals to outrage as content strategies are as old as mass media itself, which benefited from largely the same ad-based revenue structure of engagements. The unique component of social media is that these feedback loops of misinformation can be effective for nearly any set of beliefs, regardless of how fringe. The hyper-personalization of social media has a fragmenting effect, compared to mass media where only one or few perspectives are publicized. Research has shown that social media networks strongly correlate with [homophily](https://www.sciencedirect.com/science/article/abs/pii/S0047272716301001?via%3Dihub), the tendency for people to seek out like-minded others or media that espouses one's own views. Facebook released a [study](https://www.science.org/doi/abs/10.1126/science.aaa1160) arguing that "individual choices more than algorithms limit exposure to attitude-challenging content in the context of Facebook", citing a minority 24% among liberals and 35% among conservatives of cross-cutting stories that are shared on the platform. Social media thus has a reinforcing effect on the psychological tendency to engage with like-mindedness. This tendency of repetition is also characterized by Ecker et al. as instrumental to the spread of false belief: 
> This illusory truth effect arises because people use peripheral cues such as familiarity (a signal that a message has been encountered before), processing fluency (a signal that a message is either encoded or retrieved effortlessly) and cohesion (a signal that the elements of a message have references in memory that are internally consistent) as signals for truth, and the strength of these cues increases with repetition. Thus, repetition increases belief in both misinformation and facts.

Topics that resonate with many people often go viral on social media. However, individual audience members' news experience is resonant with their own ideas and values in a way that centralized publishers can never offer. In practice, the same news event could spawn a dozen different politically-motivated interpretations on social media, of which the end user is surfaced the most agreeable, homophilic one. In those increasingly rare cases where the user is offered dissenting opinions, the outcome is seldom one of consensus or mutual understanding and more often ad-hominem, sensationalized "dunks"—those same reactions that are rewarded under the umbrella term of "engagement". As [Zeynep Tufekci](https://www.technologyreview.com/2018/08/14/240325/how-social-media-took-us-from-tahrir-square-to-donald-trump/) put it, coming across opposing views is "not like reading them in a newspaper while sitting alone. It’s like hearing them from the opposing team while sitting with our fellow fans in a football stadium."

Elites and state actors alike capable of exploiting these viral and fragmented networks stand to benefit from the chaos. "The Democrats don't matter," [Steve Bannon said in a 2018 interview](https://www.vox.com/policy-and-politics/2020/1/16/20991816/impeachment-trial-trump-bannon-misinformation). "The real opposition is the media. And the way to deal with them is to flood the zone with shit." Flooding the zone is particularly effective against social media, which has few to no content regulations or centralized fact-checking mechanisms, compared to traditional media. Bad actors aiming for pure sensationalism can therein completely pollute the public's understanding of the real truth. Amidst the early-to-mid 2010s, Russian intelligence created tens of thousands of fake Twitter accounts spouting disinformation and calls to abstain from the 2016 election. These bots amplified divisive and polarizing views, creating the illusion of dissent and playing on the same inflammatory echo chambers that social media enables.

Further criticism has emerged, not against vulnerable users or even big tech corporations, but against the institution of democracy itself. Dan Williams condemns the rise of extremist, right-wing views, which he views as a "highly authoritarian, extremely corrupt political movement that attempts to overthrow [democratic elections](https://www.conspicuouscognition.com/p/is-social-media-destroying-democracyor)". However, he posits that extremist and conspiratorial views have held tacit credence throughout history but were filtered out by the elite, and that social media simply "caters to ideas and prejudices that are already popular". His closing note is that, "Perhaps democracy can't survive too much democracy."

[Jacob Siegel](https://www.tabletmag.com/sections/news/articles/guide-understanding-hoax-century-thirteen-ways-looking-disinformation) makes a different argument, that the public outcry against "disinformation" has been re-appropriated to retreat from democratic tolerance altogether. Government agencies, NGOs, and technology platforms have formed an informal but industrial-scale censorship apparatus, in which state actors outsource content moderation to quasi-private intermediaries such as the Election Integrity Project, allowing officials to shape online discourse while bypassing constitutional and legal constraints. For Siegel, the language of disinformation functions less as a defense of democracy than as a regulatory justification for permanent surveillance and centralized authority, replacing democratic contestation with machine-mediated consensus. Where Williams worries that democracy may produce too much unfiltered opinion, Siegel warns that efforts to manage that opinion risk hollowing democracy out entirely.

Williams' argument that democracy rewards misinformed, conspiratorial, and memetically contagious views overlooks how social media can be reformed to encourage truth-seeking in democratically-congruent and decentralized ways. At the same time, Siegel’s critique—that contemporary efforts to address disinformation amount to democratic backsliding driven by elite censorship--rests on the assumption that despotic oversight and control are the only available levers. Both positions implicitly treat the choice as binary: either accept an unregulated marketplace of ideas in which the most memetically competitive content dominates, or impose top-down narrative control that risks hollowing out democratic deliberation.

A third path treats social media platforms as markets whose incentive structures can be redesigned without suppressing speech. Just as economic regulation need not entail central planning, reforms to ranking, visibility, and feedback mechanisms can shape outcomes without determining permissible beliefs. For example, the community notes feature on X lets users add context or fact-check tweets; notes that have been upvoted by users with diverse views end up appended to the tweet itself as a footnote. The University of Washington [found](https://www.washington.edu/news/2025/09/18/community-notes-x-false-information-viral/) that community notes reduced public engagement with misinformation by 46% for reposts, 44% for likes, and 22% for replies. Audrey Tang, Taiwan's first Minister of Digital Affairs, has also written extensively about coordination inducing mediums of social media. Her paper with Glen Weyl describes [Prosocial Media](https://arxiv.org/abs/2502.10834)—a social media network wherein users are annotated with their community affiliations, and content is ranked based upon how well it bridges and balances ideological differences. Together, these approaches illustrate that it is possible to mitigate fragmentation and manipulation without reverting to elite gatekeeping or algorithmic surveillance. We can preserve democratic disagreement while making it harder for outrage and disinformation to dominate by default.

What the last two decades reveal, then, is not that democracy was naïvely broken by social media, but that information systems are institutions with politics embedded in their design. Social media succeeded spectacularly at dissolving centralized control over speech, yet failed to replace it with structures that support shared reality, trust, and coordinated action. In that vacuum, fragmentation enabled exploitation by states, elites, and demagogues who thrive not on persuasion but on confusion. The choice we face is not between censorship and chaos, but between leaving engagement-maximizing systems untouched and deliberately reshaping them to reward epistemic responsibility, cross-group understanding, and durable consensus. Democracy does not require uniform belief, but it does require coordinated action.
